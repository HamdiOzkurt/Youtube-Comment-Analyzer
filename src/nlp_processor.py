"""
NLP Processor - Metin Temizleme ve Normalizasyon ModÃ¼lÃ¼
YouTube yorumlarÄ± iÃ§in TÃ¼rkÃ§e NLP iÅŸleme pipeline'Ä±
data_preprocessing.ipynb Ã¶rneÄŸine gÃ¶re tasarlandÄ±
"""

import re
import pandas as pd
from typing import List, Optional, Union, Set
from timeit import default_timer as timer
from datetime import timedelta

# NLTK stopwords iÃ§in lazy loading
_stop_kelimeler = None

def get_turkish_stopwords() -> Set[str]:
    """NLTK'dan TÃ¼rkÃ§e stop words yÃ¼kle (lazy loading)"""
    global _stop_kelimeler
    
    if _stop_kelimeler is None:
        try:
            from nltk.corpus import stopwords
            _stop_kelimeler = set(stopwords.words('turkish'))
        except LookupError:
            import nltk
            print("ğŸ“¥ NLTK stopwords indiriliyor...")
            nltk.download('stopwords', quiet=True)
            from nltk.corpus import stopwords
            _stop_kelimeler = set(stopwords.words('turkish'))
        except ImportError:
            print("âš ï¸ NLTK yÃ¼klÃ¼ deÄŸil, varsayÄ±lan stop words kullanÄ±lÄ±yor")
            # Fallback stop words
            _stop_kelimeler = {
                've', 'bir', 'bu', 'da', 'de', 'iÃ§in', 'ile', 'o', 'ne', 'var',
                'ben', 'sen', 'biz', 'siz', 'onlar', 'ÅŸu', 'her', 'daha', 'Ã§ok',
                'en', 'gibi', 'kadar', 'sonra', 'Ã¶nce', 'ama', 'fakat', 'ancak',
                'ki', 'mi', 'mÄ±', 'mu', 'mÃ¼', 'ya', 'yani', 'hem', 'veya', 'ise',
                'bile', 'sadece', 'artÄ±k', 'hep', 'hiÃ§', 'olan', 'olarak'
            }
    
    return _stop_kelimeler


def preprocessing(
    series: pd.Series,
    remove_hashtag: bool = False,
    remove_mentions: bool = False,
    remove_links: bool = False,
    remove_numbers: bool = False,
    remove_short_text: bool = False,
    lowercase: bool = False,
    remove_punctuation: bool = False,
    remove_stopwords: bool = False,
    remove_rare_words: bool = False,
    remove_non_latin: bool = False,
    rare_limit: int = 5,
    custom_stopwords: Optional[Set[str]] = None,
    min_text_length: int = 3,
    verbose: bool = True
) -> pd.Series:
    """
    Pandas Series Ã¼zerinde metin Ã¶n iÅŸleme uygular.
    
    Parameters
    ----------
    series : pandas.Series
        Ä°ÅŸlenecek metin serisi.
    remove_hashtag : bool, default=False
        True ise hashtag'leri (#) kaldÄ±rÄ±r.
    remove_mentions : bool, default=False
        True ise mention'larÄ± (@) kaldÄ±rÄ±r.
    remove_links : bool, default=False
        True ise URL'leri kaldÄ±rÄ±r.
    remove_numbers : bool, default=False
        True ise sayÄ±larÄ± kaldÄ±rÄ±r.
    remove_short_text : bool, default=False
        True ise kÄ±sa kelimeleri (min_text_length'den kÄ±sa) kaldÄ±rÄ±r.
    lowercase : bool, default=False
        True ise tÃ¼m metni kÃ¼Ã§Ã¼k harfe Ã§evirir.
    remove_punctuation : bool, default=False
        True ise noktalama iÅŸaretlerini kaldÄ±rÄ±r.
    remove_stopwords : bool, default=False
        True ise TÃ¼rkÃ§e stop words'leri kaldÄ±rÄ±r.
    remove_rare_words : bool, default=False
        True ise nadir kelimeleri (rare_limit veya daha az geÃ§en) kaldÄ±rÄ±r.
    remove_non_latin : bool, default=False
        True ise ArapÃ§a, Kiril, emoji vb. karakterleri kaldÄ±rÄ±r.
    rare_limit : int, default=5
        Nadir kelime eÅŸiÄŸi.
    custom_stopwords : set veya None, default=None
        VarsayÄ±lan yerine kullanÄ±lacak Ã¶zel stop words.
    min_text_length : int, default=3
        remove_short_text=True iken minimum kelime uzunluÄŸu.
    verbose : bool, default=True
        True ise iÅŸlem sÃ¼releri yazdÄ±rÄ±lÄ±r.
        
    Returns
    -------
    pandas.Series
        Ä°ÅŸlenmiÅŸ metin serisi.
    """
    # Orijinali deÄŸiÅŸtirmemek iÃ§in kopyala
    series = series.copy()
    
    def log(msg):
        if verbose:
            print(msg)
    
    if lowercase:
        log("ğŸ”„ KÃ¼Ã§Ã¼k harfe Ã§evriliyor...")
        start = timer()
        series = series.str.lower()
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_hashtag:
        log("ğŸ”„ Hashtag'ler kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        series = series.str.replace(r'#\w+', '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_mentions:
        log("ğŸ”„ Mention'lar kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        series = series.str.replace(r'@\w+', '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_links:
        log("ğŸ”„ URL'ler kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        series = series.str.replace(r'http\S+|www\.\S+', '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_numbers:
        log("ğŸ”„ SayÄ±lar kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        series = series.str.replace(r'\d+', '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_punctuation:
        log("ğŸ”„ Noktalama iÅŸaretleri kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        series = series.str.replace(r'[^\w\s]', '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_short_text:
        log(f"ğŸ”„ KÄ±sa kelimeler kaldÄ±rÄ±lÄ±yor (uzunluk < {min_text_length})...")
        start = timer()
        pattern = r'\b\w{1,' + str(min_text_length - 1) + r'}\b'
        series = series.str.replace(pattern, '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_non_latin:
        log("ğŸ”„ Latin olmayan karakterler kaldÄ±rÄ±lÄ±yor (ArapÃ§a, Kiril, Emoji)...")
        start = timer()
        # a-z, 0-9 ve TÃ¼rkÃ§e harfler (Ã§ÄŸÄ±Ã¶ÅŸÃ¼) HARÄ°Ã‡ her ÅŸeyi sil
        series = series.str.replace(r'[^a-zA-Z0-9Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ\s]', '', regex=True)
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_stopwords:
        log("ğŸ”„ Stop words kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        stopwords_to_use = custom_stopwords if custom_stopwords else get_turkish_stopwords()
        series = series.apply(lambda x: ' '.join([
            word for word in str(x).split() 
            if word.lower() not in stopwords_to_use
        ]))
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    if remove_rare_words:
        log("ğŸ”„ Nadir kelimeler kaldÄ±rÄ±lÄ±yor...")
        start = timer()
        all_words = ' '.join(series.astype(str)).split()
        word_counts = pd.Series(all_words).value_counts()
        
        log(f"   ğŸ“Š Toplam benzersiz kelime: {len(word_counts)}")
        rare_words = word_counts[word_counts <= rare_limit]
        log(f"   ğŸ“Š Nadir kelimeler ({rare_limit} veya daha az): {len(rare_words)} "
            f"({len(rare_words)/len(word_counts)*100:.2f}%)")
        
        rare_words_set = set(rare_words.index)
        series = series.apply(lambda x: ' '.join([
            word for word in str(x).split() 
            if word not in rare_words_set
        ]))
        log(f"âœ… TamamlandÄ±: {timedelta(seconds=timer() - start)}")
    
    # Fazla boÅŸluklarÄ± temizle
    series = series.str.strip().str.replace(r'\s+', ' ', regex=True)
    
    return series


def tr_en_char_translate(series: pd.Series) -> pd.Series:
    """
    TÃ¼rkÃ§e karakterleri Ä°ngilizce karÅŸÄ±lÄ±klarÄ±na Ã§evirir.
    
    Parameters
    ----------
    series : pandas.Series
        Ã‡evrilecek metin serisi.
        
    Returns
    -------
    pandas.Series
        Ã‡evrilmiÅŸ metin serisi.
    """
    series = series.str.replace('Ä±', 'i')
    series = series.str.replace('Ã¼', 'u')
    series = series.str.replace('Ã¶', 'o')
    series = series.str.replace('ÄŸ', 'g')
    series = series.str.replace('ÅŸ', 's')
    series = series.str.replace('Ã§', 'c')
    series = series.str.replace('Ä°', 'I')
    series = series.str.replace('Ãœ', 'U')
    series = series.str.replace('Ã–', 'O')
    series = series.str.replace('Ä', 'G')
    series = series.str.replace('Å', 'S')
    series = series.str.replace('Ã‡', 'C')
    return series


class NLPProcessor:
    """TÃ¼rkÃ§e metin temizleme ve normalizasyon sÄ±nÄ±fÄ± - Basit API"""
    
    def __init__(self,
                 remove_hashtag: bool = True,
                 remove_mentions: bool = True,
                 remove_links: bool = True,
                 remove_numbers: bool = False,
                 remove_non_latin: bool = True,
                 lowercase: bool = True,
                 remove_punctuation: bool = False,
                 remove_stopwords: bool = False,
                 remove_short_text: bool = False,
                 min_text_length: int = 3,
                 custom_stopwords: Optional[Set[str]] = None):
        """
        Args:
            remove_hashtag: Hashtag'leri kaldÄ±r
            remove_mentions: Mention'larÄ± kaldÄ±r
            remove_links: URL'leri kaldÄ±r
            remove_numbers: SayÄ±larÄ± kaldÄ±r
            remove_non_latin: Emoji, ArapÃ§a vb. kaldÄ±r
            lowercase: KÃ¼Ã§Ã¼k harfe Ã§evir
            remove_punctuation: Noktalama iÅŸaretlerini kaldÄ±r
            remove_stopwords: TÃ¼rkÃ§e stop words'leri kaldÄ±r
            remove_short_text: KÄ±sa kelimeleri kaldÄ±r
            min_text_length: Minimum kelime uzunluÄŸu
            custom_stopwords: Ã–zel stop words listesi
        """
        self.remove_hashtag = remove_hashtag
        self.remove_mentions = remove_mentions
        self.remove_links = remove_links
        self.remove_numbers = remove_numbers
        self.remove_non_latin = remove_non_latin
        self.lowercase = lowercase
        self.remove_punctuation = remove_punctuation
        self.remove_stopwords = remove_stopwords
        self.remove_short_text = remove_short_text
        self.min_text_length = min_text_length
        self.custom_stopwords = custom_stopwords
        
        # Stop words'Ã¼ Ã¶nceden yÃ¼kle
        if remove_stopwords:
            self.stopwords = custom_stopwords if custom_stopwords else get_turkish_stopwords()
        else:
            self.stopwords = set()
    
    def clean_text(self, text: str) -> str:
        """Tek bir metni temizle"""
        if not text or not isinstance(text, str):
            return ""
        
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        if self.lowercase:
            text = text.lower()
        
        # Hashtag'leri kaldÄ±r
        if self.remove_hashtag:
            text = re.sub(r'#\w+', '', text)
        
        # Mention'larÄ± kaldÄ±r
        if self.remove_mentions:
            text = re.sub(r'@\w+', '', text)
        
        # URL'leri kaldÄ±r
        if self.remove_links:
            text = re.sub(r'http\S+|www\.\S+', '', text)
        
        # SayÄ±larÄ± kaldÄ±r
        if self.remove_numbers:
            text = re.sub(r'\d+', '', text)
        
        # Latin olmayan karakterleri kaldÄ±r (emoji dahil)
        if self.remove_non_latin:
            text = re.sub(r'[^a-zA-Z0-9Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ\s]', '', text)
        
        # Noktalama iÅŸaretlerini kaldÄ±r
        if self.remove_punctuation:
            text = re.sub(r'[^\w\s]', '', text)
        
        # KÄ±sa kelimeleri kaldÄ±r
        if self.remove_short_text:
            pattern = r'\b\w{1,' + str(self.min_text_length - 1) + r'}\b'
            text = re.sub(pattern, '', text)
        
        # Stop words kaldÄ±r
        if self.remove_stopwords and self.stopwords:
            words = text.split()
            text = ' '.join([w for w in words if w.lower() not in self.stopwords])
        
        # Fazla boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def process(self, text: str) -> str:
        """clean_text alias'Ä±"""
        return self.clean_text(text)
    
    def process_batch(self, texts: List[str], verbose: bool = False) -> List[str]:
        """Birden fazla metni iÅŸle"""
        if verbose:
            print(f"ğŸ”„ {len(texts)} metin iÅŸleniyor...")
        return [self.clean_text(text) for text in texts]
    
    def process_series(self, series: pd.Series, verbose: bool = True) -> pd.Series:
        """Pandas Series Ã¼zerinde iÅŸlem yap"""
        return preprocessing(
            series,
            remove_hashtag=self.remove_hashtag,
            remove_mentions=self.remove_mentions,
            remove_links=self.remove_links,
            remove_numbers=self.remove_numbers,
            remove_short_text=self.remove_short_text,
            lowercase=self.lowercase,
            remove_punctuation=self.remove_punctuation,
            remove_stopwords=self.remove_stopwords,
            remove_non_latin=self.remove_non_latin,
            custom_stopwords=self.custom_stopwords,
            min_text_length=self.min_text_length,
            verbose=verbose
        )
    
    def extract_questions(self, text: str) -> List[str]:
        """Metinden soru cÃ¼mlelerini Ã§Ä±kar"""
        if not text:
            return []
        
        questions = []
        # Soru iÅŸareti ile biten cÃ¼mleleri bul
        sentences = re.split(r'[.!]', text)
        for sentence in sentences:
            if '?' in sentence:
                parts = sentence.split('?')
                for part in parts[:-1]:
                    q = part.strip() + '?'
                    if len(q) > 5:
                        questions.append(q)
        return questions
    
    def extract_requests(self, text: str) -> List[str]:
        """Metinden talep/istek cÃ¼mlelerini Ã§Ä±kar"""
        if not text:
            return []
        
        request_patterns = [
            r'lÃ¼tfen\s+.+',
            r'.+\s+yapabilir\s*misiniz',
            r'.+\s+yapar\s*mÄ±sÄ±nÄ±z',
            r'.+\s+istiyorum',
            r'.+\s+bekliyorum',
            r'devamÄ±nÄ±\s+.+',
        ]
        
        requests = []
        text_lower = text.lower()
        
        for pattern in request_patterns:
            matches = re.findall(pattern, text_lower)
            requests.extend(matches)
        
        return list(set(requests))
    
    def get_word_frequencies(self, texts: List[str], top_n: int = 50) -> dict:
        """Kelime frekanslarÄ±nÄ± hesapla"""
        word_count = {}
        
        for text in texts:
            cleaned = self.clean_text(text)
            words = cleaned.split()
            
            for word in words:
                if len(word) >= self.min_text_length:
                    word_count[word] = word_count.get(word, 0) + 1
        
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        return dict(sorted_words[:top_n])


# ============= TEST KODU =============
if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ§¹ NLP PROCESSOR TEST")
    print("=" * 60)
    
    # Test metinleri
    test_texts = [
        "Bu video Ã§ok gÃ¼zel olmuÅŸ! ğŸ‰ğŸ”¥ https://youtube.com/watch?v=123",
        "@kanal Ã§ok beÄŸendim, devamÄ±nÄ± bekliyorum lÃ¼tfen yapÄ±n",
        "ÅarkÄ± sÃ¼per ama sanatÃ§Ä±nÄ±n sesi biraz yorgun mu?",
        "Bu tarz videolar yapabilir misiniz? Ã‡ok istiyorum! ğŸ˜",
        "Ù…Ø±Ø­Ø¨Ø§ ä½ å¥½ Merhaba dÃ¼nya! #test @user"
    ]
    
    # Pandas Series oluÅŸtur
    series = pd.Series(test_texts)
    
    print("\nğŸ“‹ PANDAS SERIES PREPROCESSING:\n")
    
    # Preprocessing uygula
    processed = preprocessing(
        series,
        lowercase=True,
        remove_mentions=True,
        remove_links=True,
        remove_non_latin=True,
        verbose=True
    )
    
    print("\nğŸ“Š SONUÃ‡LAR:")
    for i, (orig, proc) in enumerate(zip(test_texts, processed)):
        print(f"\n{i+1}. Orijinal: {orig}")
        print(f"   Temiz:    {proc}")
    
    print("\n" + "=" * 60)
    print("ğŸ”§ NLPPROCESSOR SINIFI TEST:")
    print("=" * 60)
    
    processor = NLPProcessor(
        remove_non_latin=True,
        remove_links=True,
        remove_mentions=True,
        lowercase=True,
        remove_stopwords=True
    )
    
    for text in test_texts[:3]:
        print(f"\nğŸ“ Orijinal: {text}")
        print(f"âœ¨ Temiz: {processor.clean_text(text)}")
        
        questions = processor.extract_questions(text)
        if questions:
            print(f"â“ Sorular: {questions}")
